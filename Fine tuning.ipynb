{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.1+cu118\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Tuple,Union,Dict, Any\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer,BertForTokenClassification,get_linear_schedule_with_warmup,PreTrainedTokenizer\n",
    "from torch.utils.data import DataLoader,TensorDataset,Dataset\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "# Clear GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Print PyTorch version\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading test ,train, dev text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget -O \"train.txt\" \"https://figshare.com/ndownloader/files/15320042\"\n",
    "! wget -O \"dev.txt\" \"https://figshare.com/ndownloader/files/15320048\"\n",
    "! wget -O \"test.txt\" \"https://figshare.com/ndownloader/files/15320045\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading and preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(filename: str) -> Union[pd.DataFrame, Tuple[pd.DataFrame, dict, dict]]:\n",
    "    \"\"\"\n",
    "    Preprocesses a text file containing labeled words and tags.\n",
    "\n",
    "    Args:\n",
    "        filename (str): Path to the text file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame or tuple: Processed DataFrame containing 'sentence' and 'word_labels'\n",
    "                               or a tuple (DataFrame, labels2ids, id2labels) if filename is 'train.txt'.\n",
    "    \"\"\"\n",
    "\n",
    "    # Read the text file into a DataFrame\n",
    "    dataframe= pd.read_csv(filename,sep=\"\\s+\" ,names=['Word','Tag','Tag1'],header=None,skip_blank_lines=True)\n",
    "    \n",
    "    # Combine 'Word' and 'Tag' columns based on the presence of 'Tag1'\n",
    "    dataframe['Word']=dataframe.apply(lambda row: f\"{row['Word']} {row['Tag']}\" if not pd.isna(row['Tag1']) else row['Word'], axis=1)\n",
    "    \n",
    "    # Fill missing values in 'Tag' column with values from 'Tag1'\n",
    "    dataframe['Tag'] = dataframe['Tag1'].fillna(dataframe['Tag'])\n",
    "    \n",
    "    # Extract only relevant columns\n",
    "    dataframe=dataframe[['Word','Tag']]\n",
    "\n",
    "    #Foward fill NaN values\n",
    "    if dataframe.isna().any().any():\n",
    "        dataframe=dataframe.ffill()\n",
    "\n",
    "    #Sort unique tags\n",
    "    unique_tags=sorted(dataframe.Tag.unique())\n",
    "\n",
    "    # Create dictionaries for label encoding\n",
    "    labels2ids={k:v for v,k in enumerate(unique_tags)}\n",
    "    id2labels={v:k for v,k in enumerate(unique_tags)}\n",
    "\n",
    "    #Finds sentence end\n",
    "    sentence_ends= ((dataframe['Word']=='.').cumsum()).shift(fill_value=0)\n",
    "\n",
    "    #Creates a new column 'sentence' by joining words within each sentence \n",
    "    dataframe['sentence']=dataframe.groupby(sentence_ends)['Word'].transform(lambda x: ' '.join(x))\n",
    "\n",
    "    #Creates new column 'word labels' by joining labels within each sentence \n",
    "    dataframe['word_labels']=dataframe.groupby(sentence_ends)['Tag'].transform(lambda x: ' '.join(x))\n",
    "\n",
    "    #Drop duplicate rows\n",
    "    dataframe=dataframe[['sentence','word_labels']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    if (filename=='train.txt'):\n",
    "        return dataframe,labels2ids,id2labels\n",
    "        \n",
    "    else:\n",
    "        return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_preserve_labels(row: pd.Series, tokenizer: PreTrainedTokenizer) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    Tokenizes a sentence and preserves labels for each subword.\n",
    "\n",
    "    Args:\n",
    "        row (pd.Series): A row containing 'sentence' and 'word_labels' columns.\n",
    "        tokenizer (PreTrainedTokenizer): The tokenizer object.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[str], List[str]]: Tokenized sentence and list of labels for each token.\n",
    "    \"\"\"\n",
    "    \n",
    "    sentence = row['sentence']\n",
    "    text_labels = row['word_labels']\n",
    "    \n",
    "    if not sentence or not text_labels:\n",
    "        return [],[]\n",
    "    \n",
    "    tokenized_sentence = []\n",
    "    labels = []\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    for word, label in zip(sentence.split(), text_labels.split()):\n",
    "        tokenized_word = tokenizer.tokenize(word)\n",
    "        n_subwords = len(tokenized_word)\n",
    "\n",
    "        tokenized_sentence.extend(tokenized_word)\n",
    "\n",
    "        labels.extend([label] * n_subwords)\n",
    "\n",
    "    assert len(tokenized_sentence)== len(labels)\n",
    "    return tokenized_sentence, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe: pd.DataFrame, label2ids: Dict[str, int], tokenizer: PreTrainedTokenizer, max_len: int):\n",
    "        \"\"\"\n",
    "        Custom PyTorch Dataset for Named Entity Recognition.\n",
    "\n",
    "        Args:\n",
    "            dataframe (pd.DataFrame): Pandas DataFrame containing 'sentence' and 'word_labels'.\n",
    "            label2ids (Dict[str, int]): Mapping of labels to ids.\n",
    "            tokenizer (PreTrainedTokenizer): Tokenizer object.\n",
    "            max_len (int): Maximum sequence length.\n",
    "        \"\"\"\n",
    "\n",
    "        self.data=dataframe\n",
    "        self.label2ids=label2ids\n",
    "        self.tokenizer=tokenizer\n",
    "        self.max_len=max_len\n",
    "    \n",
    "    def __getitem__(self, index: int) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Retrieves a single data point (sample) from the dataset.\n",
    "\n",
    "        Args:\n",
    "            index (int): Index of the desired data point.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, torch.Tensor]: Dictionary containing 'ids', 'mask', and 'targets'.\n",
    "        \"\"\"\n",
    "        # Get the row corresponding to the given index\n",
    "        row = self.data.iloc[index]\n",
    "\n",
    "        # Tokenize the sentence and preserve labels\n",
    "        tokenized_sentence, labels = tokenize_and_preserve_labels(row, self.tokenizer)\n",
    "        \n",
    "        # Add special tokens [CLS] and [SEP]\n",
    "        tokenized_sentence=[\"[CLS]\"]+tokenized_sentence+[\"[SEP]\"]\n",
    "\n",
    "        # Insert 'O' labels at the beginning and end\n",
    "        labels.insert(0,\"O\")\n",
    "        labels.insert(-1,\"O\")\n",
    "\n",
    "        # Truncate or pad the tokenized sentence to the specified maximum length\n",
    "        if(len(tokenized_sentence)>self.max_len):\n",
    "            tokenized_sentence=tokenized_sentence[:self.max_len]\n",
    "            labels=labels[:self.max_len]\n",
    "        else:\n",
    "            tokenized_sentence=tokenized_sentence+[\"[PAD]\" for _ in range(self.max_len-len(tokenized_sentence))]\n",
    "            labels=labels+[\"O\" for _ in range(self.max_len-len(labels))]\n",
    "\n",
    "        # Create an attention mask (1 for non-padding tokens, 0 for padding tokens)\n",
    "        attn_mask=[1 if token!='[PAD]' else 0 for token in tokenized_sentence]\n",
    "\n",
    "        # Convert tokens to ids using the tokenizer\n",
    "        ids=self.tokenizer.convert_tokens_to_ids(tokenized_sentence)\n",
    "\n",
    "        # Map label strings to label ids using the provided mapping\n",
    "        label_ids = [self.label2ids[label] for label in labels]\n",
    "        \n",
    "        # Return the data as a dictionary of torch Tensors    \n",
    "        return{\n",
    "            'ids' : torch.tensor(ids,dtype=torch.long),\n",
    "            'mask' : torch.tensor(attn_mask,dtype=torch.long),\n",
    "            'targets': torch.tensor(label_ids,dtype=torch.long)\n",
    "        }\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 128\n",
    "TRAIN_BATCH_SIZE=4\n",
    "VAL_BATCH_SIZE=2\n",
    "EPOCHS = 1\n",
    "MAX_GRAD_NORM = 10\n",
    "LEARNING_RATE=1\n",
    "\n",
    "# Initialize the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Preprocess the training data\n",
    "train_data,label2ids,ids2labels=preprocess('train.txt')\n",
    "\n",
    "# Preprocess the validation data\n",
    "val_data=preprocess('dev.txt')\n",
    "\n",
    "# Preprocess the test data\n",
    "test_data=preprocess('test.txt')\n",
    "\n",
    "# Create CustomDataset instances for training, validation, and test datasets\n",
    "train_vector=CustomDataset(train_data,label2ids,tokenizer,MAX_LEN)\n",
    "val_vector=CustomDataset(val_data,label2ids,tokenizer,MAX_LEN)\n",
    "test_vector=CustomDataset(test_data,label2ids,tokenizer,MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the first item in the training dataset\n",
    "sample_item = train_vector[0]\n",
    "\n",
    "# Extract 'ids' and 'targets' from the sample item\n",
    "sample_ids = sample_item['ids'][:100]\n",
    "sample_targets = sample_item['targets'][:100]\n",
    "\n",
    "# Iterate through tokens and labels, printing them\n",
    "for token, label in zip(tokenizer.convert_ids_to_tokens(sample_ids), sample_targets):\n",
    "    # Retrieve the label name using the provided mapping\n",
    "    label_name = ids2labels[label.item()]\n",
    "\n",
    "    # Print token and corresponding label\n",
    "    #print('{0:10}  {1}'.format(token, label_name))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=15, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the BERT model for token classification\n",
    "model = BertForTokenClassification.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    num_labels=len(ids2labels),\n",
    "    id2label=ids2labels,\n",
    "    label2id=label2ids\n",
    ")\n",
    "\n",
    "# Move the model to the specified device (GPU if available, otherwise CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader for training dataset\n",
    "train_loader = DataLoader(train_vector, batch_size=TRAIN_BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "\n",
    "# Create DataLoader for validation dataset\n",
    "val_loader = DataLoader(val_vector, batch_size=VAL_BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "\n",
    "# Create DataLoader for test dataset\n",
    "test_loader = DataLoader(test_vector, batch_size=VAL_BATCH_SIZE, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.7569, device='cuda:0', grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract 'ids', 'mask', and 'targets' from the first item in the training dataset\n",
    "sample_item = train_vector[0]\n",
    "ids = sample_item['ids'].unsqueeze(0)\n",
    "mask = sample_item['mask'].unsqueeze(0)\n",
    "targets = sample_item['targets'].unsqueeze(0)\n",
    "\n",
    "# Move tensors to the specified device (GPU if available, otherwise CPU)\n",
    "ids = ids.to(device)\n",
    "mask = mask.to(device)\n",
    "targets = targets.to(device)\n",
    "\n",
    "# Forward pass through the model\n",
    "outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n",
    "\n",
    "# Extract the initial loss from the model outputs\n",
    "initial_loss = outputs.loss\n",
    "initial_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "Training Loss: 0.1707\n",
      "Training Precision: 0.8432\n",
      "Training Recall: 0.8499\n",
      "Training F1 Score: 0.8446\n"
     ]
    }
   ],
   "source": [
    "def train(model: BertForTokenClassification,\n",
    "          optimizer: AdamW,\n",
    "          scheduler: torch.optim.lr_scheduler,\n",
    "          training_loader: DataLoader,\n",
    "          device: torch.device,\n",
    "          epochs: int,\n",
    "          max_grad_norm: float) -> None:\n",
    "    \"\"\"\n",
    "    Train the BERT-based token classification model.\n",
    "\n",
    "    Args:\n",
    "        model (BertForTokenClassification): The token classification model.\n",
    "        optimizer (AdamW): The optimizer for updating model parameters.\n",
    "        scheduler (torch.optim.lr_scheduler): Learning rate scheduler.\n",
    "        training_loader (DataLoader): DataLoader for the training dataset.\n",
    "        device (torch.device): The device to use for training (GPU or CPU).\n",
    "        epochs (int): Number of training epochs.\n",
    "        max_grad_norm (float): Maximum gradient norm for gradient clipping.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        tr_loss, tr_preds, tr_labels = 0, [], []\n",
    "        nb_tr_steps = 0\n",
    "\n",
    "        for idx, batch in enumerate(training_loader):\n",
    "            ids = batch['ids'].to(device, dtype=torch.long)\n",
    "            mask = batch['mask'].to(device, dtype=torch.long)\n",
    "            targets = batch['targets'].to(device, dtype=torch.long)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n",
    "            loss, tr_logits = outputs.loss, outputs.logits\n",
    "            tr_loss += loss.item()\n",
    "\n",
    "            nb_tr_steps += 1\n",
    "\n",
    "            flattened_targets = targets.view(-1)\n",
    "            active_logits = tr_logits.view(-1, model.config.num_labels)\n",
    "            flattened_predictions = torch.argmax(active_logits, axis=1)\n",
    "            active_accuracy = mask.view(-1) == 1\n",
    "            targets = torch.masked_select(flattened_targets, active_accuracy)\n",
    "            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
    "\n",
    "            tr_preds.extend(predictions.cpu().numpy())\n",
    "            tr_labels.extend(targets.cpu().numpy())\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                parameters=model.parameters(), max_norm=max_grad_norm\n",
    "            )\n",
    "\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "        tr_loss /= nb_tr_steps\n",
    "\n",
    "        # Calculate precision, recall, and F1 score\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(tr_labels, tr_preds, average='weighted', zero_division=1)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "        print(f\"Training Loss: {tr_loss:.4f}\")\n",
    "        print(f\"Training Precision: {precision:.4f}\")\n",
    "        print(f\"Training Recall: {recall:.4f}\")\n",
    "        print(f\"Training F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Example usage:\n",
    "optimizer = AdamW(model.parameters(), lr=5e-05)\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "train(model, optimizer, scheduler, train_loader, device, epochs=EPOCHS, max_grad_norm=MAX_GRAD_NORM)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Evaluation:\n",
      "Average Loss: 0.0976\n",
      "Precision: 0.9035\n",
      "Recall: 0.9031\n",
      "F1 Score: 0.9024\n",
      "\n",
      "Test Evaluation:\n",
      "Average Loss: 0.1062\n",
      "Precision: 0.8999\n",
      "Recall: 0.8989\n",
      "F1 Score: 0.8979\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model: torch.nn.Module,\n",
    "             dataloader: DataLoader,\n",
    "             device: torch.device) -> None:\n",
    "    \"\"\"\n",
    "    Evaluate the BERT-based token classification model.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The token classification model.\n",
    "        dataloader (DataLoader): DataLoader for the evaluation dataset.\n",
    "        device (torch.device): The device to use for evaluation (GPU or CPU).\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            ids, mask, targets = batch['ids'].to(device), batch['mask'].to(device), batch['targets'].to(device)\n",
    "\n",
    "            outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n",
    "            batch_loss, logits = outputs.loss, outputs.logits\n",
    "\n",
    "            loss += batch_loss.item()\n",
    "\n",
    "            flattened_targets = targets.view(-1)\n",
    "            active_logits = logits.view(-1, model.config.num_labels)\n",
    "            flattened_predictions = torch.argmax(active_logits, axis=1)\n",
    "            active_accuracy = mask.view(-1) == 1\n",
    "            targets = torch.masked_select(flattened_targets, active_accuracy)\n",
    "            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
    "\n",
    "            all_preds.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(targets.cpu().numpy())\n",
    "\n",
    "    average_loss = loss / len(dataloader)\n",
    "\n",
    "    # Calculate precision, recall, and F1 score\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted',zero_division=1)\n",
    "\n",
    "    print(f\"Average Loss: {average_loss:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Example usage:\n",
    "print(\"Validation Evaluation:\")\n",
    "validate=evaluate(model, val_loader, device)\n",
    "\n",
    "print(\"\\nTest Evaluation:\")\n",
    "test=evaluate(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Vanadium oxide', 'MAT'), ('nanotubes', 'DSC'), ('are', 'O'), ('promising', 'O'), ('for', 'O'), ('gas sensors', 'APL'), ('.', 'O')]\n"
     ]
    }
   ],
   "source": [
    "def predict_sentence(model, tokenizer, sentence, device, max_len=128):\n",
    "    \"\"\"\n",
    "    Predicts labels for a given sentence using the fine-tuned BERT model.\n",
    "\n",
    "    Args:\n",
    "        model (BertForTokenClassification): The fine-tuned BERT model.\n",
    "        tokenizer (BertTokenizer): The tokenizer used during training.\n",
    "        sentence (str): The input sentence.\n",
    "        device (torch.device): The device to use for prediction (GPU or CPU).\n",
    "        max_len (int): Maximum sequence length.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[str, List[Tuple[str, str]]]: A tuple containing the original sentence and a list of word-level predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    # Setting model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Tokenizes sentence\n",
    "    inputs = tokenizer(sentence, padding='max_length', truncation=True, max_length=max_len, return_tensors=\"pt\")\n",
    "    ids = inputs[\"input_ids\"].to(device)\n",
    "    mask = inputs[\"attention_mask\"].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(ids, mask)\n",
    "        logits = outputs.logits  # Use 'logits' instead of 'outputs[0]'\n",
    "\n",
    "    active_logits = logits.view(-1, model.config.num_labels)\n",
    "    flattened_pred = torch.argmax(active_logits, axis=1)\n",
    "\n",
    "    tokens = tokenizer.convert_ids_to_tokens(ids.squeeze().tolist())\n",
    "    token_pred = [model.config.id2label[i] for i in flattened_pred.cpu().numpy()]\n",
    "    wp_preds = list(zip(tokens, token_pred))\n",
    "\n",
    "    word_pred = []\n",
    "    current_label = None\n",
    "    current_word = \"\"\n",
    "\n",
    "    for pair in wp_preds:\n",
    "        if pair[0] in ['[CLS]', '[SEP]', '[PAD]']:\n",
    "            continue\n",
    "        \n",
    "        label=pair[1].split('-')[-1] if '-' in pair[1] else pair[1]\n",
    "\n",
    "        if current_label is None:\n",
    "            current_label=label\n",
    "            current_word=pair[0]\n",
    "        elif label==current_label and current_label !='O':\n",
    "            if pair[0].startswith(\"##\"):\n",
    "                current_word += pair[0][2:]\n",
    "                if current_word in sentence.lower().split(' '):\n",
    "                    original_word_index = sentence.lower().split(' ').index(current_word.lower())\n",
    "                    original_word = sentence.split(' ')[original_word_index]\n",
    "                    current_word = original_word\n",
    "                    \n",
    "            else:\n",
    "                current_word += \" \" + pair[0]\n",
    "\n",
    "        else:\n",
    "            word_pred.append((current_word,current_label))\n",
    "            current_label= label\n",
    "            current_word=pair[0]\n",
    "        \n",
    "    if current_label is not None:\n",
    "        word_pred.append((current_word,current_label))\n",
    "\n",
    "    return word_pred\n",
    "            \n",
    "# Example usage:\n",
    "test_sentence = \"Vanadium oxide nanotubes are promising for gas sensors.\"\n",
    "word_pred = predict_sentence(model, tokenizer, test_sentence, device)\n",
    "\n",
    "print(word_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
