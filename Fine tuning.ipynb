{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.1+cu118\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Tuple,Union,Dict, Any\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer,BertForTokenClassification,get_linear_schedule_with_warmup,PreTrainedTokenizer\n",
    "from torch.utils.data import DataLoader,TensorDataset,Dataset\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "# Clear GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Print PyTorch version\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading test ,train, dev text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "! wget -O \"train.txt\" \"https://figshare.com/ndownloader/files/15320042\"\n",
    "! wget -O \"dev.txt\" \"https://figshare.com/ndownloader/files/15320048\"\n",
    "! wget -O \"test.txt\" \"https://figshare.com/ndownloader/files/15320045\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading and preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(filename: str) -> Union[pd.DataFrame, Tuple[pd.DataFrame, dict, dict]]:\n",
    "    \"\"\"\n",
    "    Preprocesses a text file containing labeled words and tags.\n",
    "\n",
    "    Args:\n",
    "        filename (str): Path to the text file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame or tuple: Processed DataFrame containing 'sentence' and 'word_labels'\n",
    "                               or a tuple (DataFrame, labels2ids, id2labels) if filename is 'train.txt'.\n",
    "    \"\"\"\n",
    "\n",
    "    # Read the text file into a DataFrame\n",
    "    dataframe= pd.read_csv(filename,sep=\"\\s+\" ,names=['Word','Tag','Tag1'],header=None,skip_blank_lines=True)\n",
    "    \n",
    "    # Combine 'Word' and 'Tag' columns based on the presence of 'Tag1'\n",
    "    dataframe['Word']=dataframe.apply(lambda row: f\"{row['Word']} {row['Tag']}\" if not pd.isna(row['Tag1']) else row['Word'], axis=1)\n",
    "    \n",
    "    # Fill missing values in 'Tag' column with values from 'Tag1'\n",
    "    dataframe['Tag'] = dataframe['Tag1'].fillna(dataframe['Tag'])\n",
    "    \n",
    "    # Extract only relevant columns\n",
    "    dataframe=dataframe[['Word','Tag']]\n",
    "\n",
    "    #Foward fill NaN values\n",
    "    if dataframe.isna().any().any():\n",
    "        dataframe=dataframe.ffill()\n",
    "\n",
    "    #Sort unique tags\n",
    "    unique_tags=sorted(dataframe.Tag.unique())\n",
    "\n",
    "    # Create dictionaries for label encoding\n",
    "    labels2ids={k:v for v,k in enumerate(unique_tags)}\n",
    "    id2labels={v:k for v,k in enumerate(unique_tags)}\n",
    "\n",
    "    #Finds sentence end\n",
    "    sentence_ends= ((dataframe['Word']=='.').cumsum()).shift(fill_value=0)\n",
    "\n",
    "    #Creates a new column 'sentence' by joining words within each sentence \n",
    "    dataframe['sentence']=dataframe.groupby(sentence_ends)['Word'].transform(lambda x: ' '.join(x))\n",
    "\n",
    "    #Creates new column 'word labels' by joining labels within each sentence \n",
    "    dataframe['word_labels']=dataframe.groupby(sentence_ends)['Tag'].transform(lambda x: ' '.join(x))\n",
    "\n",
    "    #Drop duplicate rows\n",
    "    dataframe=dataframe[['sentence','word_labels']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    if (filename=='train.txt'):\n",
    "        return dataframe,labels2ids,id2labels\n",
    "        \n",
    "    else:\n",
    "        return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_preserve_labels(row: pd.Series, tokenizer: PreTrainedTokenizer) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    Tokenizes a sentence and preserves labels for each subword.\n",
    "\n",
    "    Args:\n",
    "        row (pd.Series): A row containing 'sentence' and 'word_labels' columns.\n",
    "        tokenizer (PreTrainedTokenizer): The tokenizer object.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[str], List[str]]: Tokenized sentence and list of labels for each token.\n",
    "    \"\"\"\n",
    "    \n",
    "    sentence = row['sentence']\n",
    "    text_labels = row['word_labels']\n",
    "    \n",
    "    if not sentence or not text_labels:\n",
    "        return [],[]\n",
    "    \n",
    "    tokenized_sentence = []\n",
    "    labels = []\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    for word, label in zip(sentence.split(), text_labels.split()):\n",
    "        tokenized_word = tokenizer.tokenize(word)\n",
    "        n_subwords = len(tokenized_word)\n",
    "\n",
    "        tokenized_sentence.extend(tokenized_word)\n",
    "\n",
    "        labels.extend([label] * n_subwords)\n",
    "\n",
    "    assert len(tokenized_sentence)== len(labels)\n",
    "    return tokenized_sentence, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe: pd.DataFrame, label2ids: Dict[str, int], tokenizer: PreTrainedTokenizer, max_len: int):\n",
    "        \"\"\"\n",
    "        Custom PyTorch Dataset for Named Entity Recognition.\n",
    "\n",
    "        Args:\n",
    "            dataframe (pd.DataFrame): Pandas DataFrame containing 'sentence' and 'word_labels'.\n",
    "            label2ids (Dict[str, int]): Mapping of labels to ids.\n",
    "            tokenizer (PreTrainedTokenizer): Tokenizer object.\n",
    "            max_len (int): Maximum sequence length.\n",
    "        \"\"\"\n",
    "\n",
    "        self.data=dataframe\n",
    "        self.label2ids=label2ids\n",
    "        self.tokenizer=tokenizer\n",
    "        self.max_len=max_len\n",
    "    \n",
    "    def __getitem__(self, index: int) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Retrieves a single data point (sample) from the dataset.\n",
    "\n",
    "        Args:\n",
    "            index (int): Index of the desired data point.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, torch.Tensor]: Dictionary containing 'ids', 'mask', and 'targets'.\n",
    "        \"\"\"\n",
    "        # Get the row corresponding to the given index\n",
    "        row = self.data.iloc[index]\n",
    "\n",
    "        # Tokenize the sentence and preserve labels\n",
    "        tokenized_sentence, labels = tokenize_and_preserve_labels(row, self.tokenizer)\n",
    "        \n",
    "        # Add special tokens [CLS] and [SEP]\n",
    "        tokenized_sentence=[\"[CLS]\"]+tokenized_sentence+[\"[SEP]\"]\n",
    "\n",
    "        # Insert 'O' labels at the beginning and end\n",
    "        labels.insert(0,\"O\")\n",
    "        labels.insert(-1,\"O\")\n",
    "\n",
    "        # Truncate or pad the tokenized sentence to the specified maximum length\n",
    "        if(len(tokenized_sentence)>self.max_len):\n",
    "            tokenized_sentence=tokenized_sentence[:self.max_len]\n",
    "            labels=labels[:self.max_len]\n",
    "        else:\n",
    "            tokenized_sentence=tokenized_sentence+[\"[PAD]\" for _ in range(self.max_len-len(tokenized_sentence))]\n",
    "            labels=labels+[\"O\" for _ in range(self.max_len-len(labels))]\n",
    "\n",
    "        # Create an attention mask (1 for non-padding tokens, 0 for padding tokens)\n",
    "        attn_mask=[1 if token!='[PAD]' else 0 for token in tokenized_sentence]\n",
    "\n",
    "        # Convert tokens to ids using the tokenizer\n",
    "        ids=self.tokenizer.convert_tokens_to_ids(tokenized_sentence)\n",
    "\n",
    "        # Map label strings to label ids using the provided mapping\n",
    "        label_ids = [self.label2ids[label] for label in labels]\n",
    "        \n",
    "        # Return the data as a dictionary of torch Tensors    \n",
    "        return{\n",
    "            'ids' : torch.tensor(ids,dtype=torch.long),\n",
    "            'mask' : torch.tensor(attn_mask,dtype=torch.long),\n",
    "            'targets': torch.tensor(label_ids,dtype=torch.long)\n",
    "        }\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'train.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\amall\\OneDrive\\Desktop\\JOB\\Lass KG\\1. Task\\mms-lm\\Notebook\\Fine tuning.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/amall/OneDrive/Desktop/JOB/Lass%20KG/1.%20Task/mms-lm/Notebook/Fine%20tuning.ipynb#W6sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m BertTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m'\u001b[39m\u001b[39mbert-base-uncased\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/amall/OneDrive/Desktop/JOB/Lass%20KG/1.%20Task/mms-lm/Notebook/Fine%20tuning.ipynb#W6sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# Preprocess the training data\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/amall/OneDrive/Desktop/JOB/Lass%20KG/1.%20Task/mms-lm/Notebook/Fine%20tuning.ipynb#W6sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m train_data,label2ids,ids2labels\u001b[39m=\u001b[39mpreprocess(\u001b[39m'\u001b[39;49m\u001b[39mtrain.txt\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/amall/OneDrive/Desktop/JOB/Lass%20KG/1.%20Task/mms-lm/Notebook/Fine%20tuning.ipynb#W6sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# Preprocess the validation data\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/amall/OneDrive/Desktop/JOB/Lass%20KG/1.%20Task/mms-lm/Notebook/Fine%20tuning.ipynb#W6sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m val_data\u001b[39m=\u001b[39mpreprocess(\u001b[39m'\u001b[39m\u001b[39mdev.txt\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\amall\\OneDrive\\Desktop\\JOB\\Lass KG\\1. Task\\mms-lm\\Notebook\\Fine tuning.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/amall/OneDrive/Desktop/JOB/Lass%20KG/1.%20Task/mms-lm/Notebook/Fine%20tuning.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/amall/OneDrive/Desktop/JOB/Lass%20KG/1.%20Task/mms-lm/Notebook/Fine%20tuning.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mPreprocesses a text file containing labeled words and tags.\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/amall/OneDrive/Desktop/JOB/Lass%20KG/1.%20Task/mms-lm/Notebook/Fine%20tuning.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/amall/OneDrive/Desktop/JOB/Lass%20KG/1.%20Task/mms-lm/Notebook/Fine%20tuning.ipynb#W6sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m                           or a tuple (DataFrame, labels2ids, id2labels) if filename is 'train.txt'.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/amall/OneDrive/Desktop/JOB/Lass%20KG/1.%20Task/mms-lm/Notebook/Fine%20tuning.ipynb#W6sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/amall/OneDrive/Desktop/JOB/Lass%20KG/1.%20Task/mms-lm/Notebook/Fine%20tuning.ipynb#W6sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# Read the text file into a DataFrame\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/amall/OneDrive/Desktop/JOB/Lass%20KG/1.%20Task/mms-lm/Notebook/Fine%20tuning.ipynb#W6sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m dataframe\u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(filename,sep\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39ms+\u001b[39;49m\u001b[39m\"\u001b[39;49m ,names\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mWord\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mTag\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mTag1\u001b[39;49m\u001b[39m'\u001b[39;49m],header\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,skip_blank_lines\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/amall/OneDrive/Desktop/JOB/Lass%20KG/1.%20Task/mms-lm/Notebook/Fine%20tuning.ipynb#W6sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# Combine 'Word' and 'Tag' columns based on the presence of 'Tag1'\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/amall/OneDrive/Desktop/JOB/Lass%20KG/1.%20Task/mms-lm/Notebook/Fine%20tuning.ipynb#W6sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m dataframe[\u001b[39m'\u001b[39m\u001b[39mWord\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m=\u001b[39mdataframe\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m row: \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mrow[\u001b[39m'\u001b[39m\u001b[39mWord\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mrow[\u001b[39m'\u001b[39m\u001b[39mTag\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m pd\u001b[39m.\u001b[39misna(row[\u001b[39m'\u001b[39m\u001b[39mTag1\u001b[39m\u001b[39m'\u001b[39m]) \u001b[39melse\u001b[39;00m row[\u001b[39m'\u001b[39m\u001b[39mWord\u001b[39m\u001b[39m'\u001b[39m], axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\amall\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    944\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[0;32m    945\u001b[0m )\n\u001b[0;32m    946\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 948\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\amall\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    608\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    610\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 611\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    613\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[0;32m    614\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\amall\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m   1447\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1448\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[1;32mc:\\Users\\amall\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1703\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[0;32m   1704\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> 1705\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[0;32m   1706\u001b[0m     f,\n\u001b[0;32m   1707\u001b[0m     mode,\n\u001b[0;32m   1708\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1709\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1710\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m   1711\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[0;32m   1712\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1713\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1714\u001b[0m )\n\u001b[0;32m   1715\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1716\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\amall\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    858\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    859\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    860\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[0;32m    862\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[1;32m--> 863\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[0;32m    864\u001b[0m             handle,\n\u001b[0;32m    865\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    866\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    867\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m    868\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    869\u001b[0m         )\n\u001b[0;32m    870\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m    872\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train.txt'"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 128\n",
    "TRAIN_BATCH_SIZE=4\n",
    "VAL_BATCH_SIZE=2\n",
    "EPOCHS = 1\n",
    "MAX_GRAD_NORM = 10\n",
    "LEARNING_RATE=1\n",
    "\n",
    "# Initialize the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Preprocess the training data\n",
    "train_data,label2ids,ids2labels=preprocess('train.txt')\n",
    "\n",
    "# Preprocess the validation data\n",
    "val_data=preprocess('dev.txt')\n",
    "\n",
    "# Preprocess the test data\n",
    "test_data=preprocess('test.txt')\n",
    "\n",
    "# Create CustomDataset instances for training, validation, and test datasets\n",
    "train_vector=CustomDataset(train_data,label2ids,tokenizer,MAX_LEN)\n",
    "val_vector=CustomDataset(val_data,label2ids,tokenizer,MAX_LEN)\n",
    "test_vector=CustomDataset(test_data,label2ids,tokenizer,MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]       O\n",
      "variable    O\n",
      "temperature  O\n",
      "electron    B-CMT\n",
      "para        I-CMT\n",
      "##ma        I-CMT\n",
      "##gne       I-CMT\n",
      "##tic       I-CMT\n",
      "resonance   I-CMT\n",
      "studies     O\n",
      "of          O\n",
      "the         O\n",
      "ni          B-MAT\n",
      "##z         B-MAT\n",
      "##n         B-MAT\n",
      "fe          I-MAT\n",
      "##rri       I-MAT\n",
      "##te        I-MAT\n",
      "/           O\n",
      "o           B-MAT\n",
      "##2         B-MAT\n",
      "##si        B-MAT\n",
      "nano        B-DSC\n",
      "##com       B-DSC\n",
      "##po        B-DSC\n",
      "##sit       B-DSC\n",
      "##e         B-DSC\n",
      "effects     O\n",
      "of          O\n",
      "the         O\n",
      "si          B-MAT\n",
      "##lica      B-MAT\n",
      "content     O\n",
      "and         O\n",
      "temperature  O\n",
      "on          O\n",
      "the         O\n",
      "magnetic    B-PRO\n",
      "properties  I-PRO\n",
      "of          O\n",
      "fe          B-MAT\n",
      "##4         B-MAT\n",
      "##nio       B-MAT\n",
      "##8         B-MAT\n",
      "##z         B-MAT\n",
      "##n         B-MAT\n",
      "/           O\n",
      "o           B-MAT\n",
      "##2         B-MAT\n",
      "##si        B-MAT\n",
      "nano        B-DSC\n",
      "##com       B-DSC\n",
      "##po        B-DSC\n",
      "##sit       B-DSC\n",
      "##es        B-DSC\n",
      "have        O\n",
      "been        O\n",
      "studied     O\n",
      "by          O\n",
      "electron    B-CMT\n",
      "para        I-CMT\n",
      "##ma        I-CMT\n",
      "##gne       I-CMT\n",
      "##tic       I-CMT\n",
      "resonance   I-CMT\n",
      "(           O\n",
      "ep          B-CMT\n",
      "##r         B-CMT\n",
      ")           O\n",
      "technique   O\n",
      ".           O\n",
      "[SEP]       O\n",
      "[PAD]       O\n",
      "[PAD]       O\n",
      "[PAD]       O\n",
      "[PAD]       O\n",
      "[PAD]       O\n",
      "[PAD]       O\n",
      "[PAD]       O\n",
      "[PAD]       O\n",
      "[PAD]       O\n",
      "[PAD]       O\n",
      "[PAD]       O\n",
      "[PAD]       O\n",
      "[PAD]       O\n",
      "[PAD]       O\n",
      "[PAD]       O\n",
      "[PAD]       O\n",
      "[PAD]       O\n",
      "[PAD]       O\n",
      "[PAD]       O\n",
      "[PAD]       O\n",
      "[PAD]       O\n",
      "[PAD]       O\n",
      "[PAD]       O\n",
      "[PAD]       O\n",
      "[PAD]       O\n",
      "[PAD]       O\n",
      "[PAD]       O\n",
      "[PAD]       O\n"
     ]
    }
   ],
   "source": [
    "# Access the first item in the training dataset\n",
    "sample_item = train_vector[0]\n",
    "\n",
    "# Extract 'ids' and 'targets' from the sample item\n",
    "sample_ids = sample_item['ids'][:100]\n",
    "sample_targets = sample_item['targets'][:100]\n",
    "\n",
    "# Iterate through tokens and labels, printing them\n",
    "for token, label in zip(tokenizer.convert_ids_to_tokens(sample_ids), sample_targets):\n",
    "    # Retrieve the label name using the provided mapping\n",
    "    label_name = ids2labels[label.item()]\n",
    "\n",
    "    # Print token and corresponding label\n",
    "    #print('{0:10}  {1}'.format(token, label_name))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=15, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the BERT model for token classification\n",
    "model = BertForTokenClassification.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    num_labels=len(ids2labels),\n",
    "    id2label=ids2labels,\n",
    "    label2id=label2ids\n",
    ")\n",
    "\n",
    "# Move the model to the specified device (GPU if available, otherwise CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader for training dataset\n",
    "train_loader = DataLoader(train_vector, batch_size=TRAIN_BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "\n",
    "# Create DataLoader for validation dataset\n",
    "val_loader = DataLoader(val_vector, batch_size=VAL_BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "\n",
    "# Create DataLoader for test dataset\n",
    "test_loader = DataLoader(test_vector, batch_size=VAL_BATCH_SIZE, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.7627, device='cuda:0', grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract 'ids', 'mask', and 'targets' from the first item in the training dataset\n",
    "sample_item = train_vector[0]\n",
    "ids = sample_item['ids'].unsqueeze(0)\n",
    "mask = sample_item['mask'].unsqueeze(0)\n",
    "targets = sample_item['targets'].unsqueeze(0)\n",
    "\n",
    "# Move tensors to the specified device (GPU if available, otherwise CPU)\n",
    "ids = ids.to(device)\n",
    "mask = mask.to(device)\n",
    "targets = targets.to(device)\n",
    "\n",
    "# Forward pass through the model\n",
    "outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n",
    "\n",
    "# Extract the initial loss from the model outputs\n",
    "initial_loss = outputs.loss\n",
    "initial_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "Training Loss: 0.1672\n",
      "Training Precision: 0.8476\n",
      "Training Recall: 0.8540\n",
      "Training F1 Score: 0.8492\n"
     ]
    }
   ],
   "source": [
    "def train(model: BertForTokenClassification,\n",
    "          optimizer: AdamW,\n",
    "          scheduler: torch.optim.lr_scheduler,\n",
    "          training_loader: DataLoader,\n",
    "          device: torch.device,\n",
    "          epochs: int,\n",
    "          max_grad_norm: float) -> None:\n",
    "    \"\"\"\n",
    "    Train the BERT-based token classification model.\n",
    "\n",
    "    Args:\n",
    "        model (BertForTokenClassification): The token classification model.\n",
    "        optimizer (AdamW): The optimizer for updating model parameters.\n",
    "        scheduler (torch.optim.lr_scheduler): Learning rate scheduler.\n",
    "        training_loader (DataLoader): DataLoader for the training dataset.\n",
    "        device (torch.device): The device to use for training (GPU or CPU).\n",
    "        epochs (int): Number of training epochs.\n",
    "        max_grad_norm (float): Maximum gradient norm for gradient clipping.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        tr_loss, tr_preds, tr_labels = 0, [], []\n",
    "        nb_tr_steps = 0\n",
    "\n",
    "        for idx, batch in enumerate(training_loader):\n",
    "            ids = batch['ids'].to(device, dtype=torch.long)\n",
    "            mask = batch['mask'].to(device, dtype=torch.long)\n",
    "            targets = batch['targets'].to(device, dtype=torch.long)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n",
    "            loss, tr_logits = outputs.loss, outputs.logits\n",
    "            tr_loss += loss.item()\n",
    "\n",
    "            nb_tr_steps += 1\n",
    "\n",
    "            flattened_targets = targets.view(-1)\n",
    "            active_logits = tr_logits.view(-1, model.config.num_labels)\n",
    "            flattened_predictions = torch.argmax(active_logits, axis=1)\n",
    "            active_accuracy = mask.view(-1) == 1\n",
    "            targets = torch.masked_select(flattened_targets, active_accuracy)\n",
    "            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
    "\n",
    "            tr_preds.extend(predictions.cpu().numpy())\n",
    "            tr_labels.extend(targets.cpu().numpy())\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                parameters=model.parameters(), max_norm=max_grad_norm\n",
    "            )\n",
    "\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "        tr_loss /= nb_tr_steps\n",
    "\n",
    "        # Calculate precision, recall, and F1 score\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(tr_labels, tr_preds, average='weighted', zero_division=1)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "        print(f\"Training Loss: {tr_loss:.4f}\")\n",
    "        print(f\"Training Precision: {precision:.4f}\")\n",
    "        print(f\"Training Recall: {recall:.4f}\")\n",
    "        print(f\"Training F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Example usage:\n",
    "optimizer = AdamW(model.parameters(), lr=5e-05)\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "train(model, optimizer, scheduler, train_loader, device, epochs=EPOCHS, max_grad_norm=MAX_GRAD_NORM)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Evaluation:\n",
      "Average Loss: 0.1022\n",
      "Precision: 0.9000\n",
      "Recall: 0.9006\n",
      "F1 Score: 0.8995\n",
      "\n",
      "Test Evaluation:\n",
      "Average Loss: 0.1058\n",
      "Precision: 0.9006\n",
      "Recall: 0.9007\n",
      "F1 Score: 0.8992\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model: torch.nn.Module,\n",
    "             dataloader: DataLoader,\n",
    "             device: torch.device) -> None:\n",
    "    \"\"\"\n",
    "    Evaluate the BERT-based token classification model.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The token classification model.\n",
    "        dataloader (DataLoader): DataLoader for the evaluation dataset.\n",
    "        device (torch.device): The device to use for evaluation (GPU or CPU).\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            ids, mask, targets = batch['ids'].to(device), batch['mask'].to(device), batch['targets'].to(device)\n",
    "\n",
    "            outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n",
    "            batch_loss, logits = outputs.loss, outputs.logits\n",
    "\n",
    "            loss += batch_loss.item()\n",
    "\n",
    "            flattened_targets = targets.view(-1)\n",
    "            active_logits = logits.view(-1, model.config.num_labels)\n",
    "            flattened_predictions = torch.argmax(active_logits, axis=1)\n",
    "            active_accuracy = mask.view(-1) == 1\n",
    "            targets = torch.masked_select(flattened_targets, active_accuracy)\n",
    "            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
    "\n",
    "            all_preds.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(targets.cpu().numpy())\n",
    "\n",
    "    average_loss = loss / len(dataloader)\n",
    "\n",
    "    # Calculate precision, recall, and F1 score\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted',zero_division=1)\n",
    "\n",
    "    print(f\"Average Loss: {average_loss:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Example usage:\n",
    "print(\"Validation Evaluation:\")\n",
    "validate=evaluate(model, val_loader, device)\n",
    "\n",
    "print(\"\\nTest Evaluation:\")\n",
    "test=evaluate(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Vanadium oxide', 'MAT'), ('nanotubes', 'DSC'), ('are', 'O'), ('promising', 'O'), ('for', 'O'), ('gas sensors', 'APL'), ('.', 'O')]\n"
     ]
    }
   ],
   "source": [
    "def predict_sentence(model, tokenizer, sentence, device, max_len=128):\n",
    "    \"\"\"\n",
    "    Predicts labels for a given sentence using the fine-tuned BERT model.\n",
    "\n",
    "    Args:\n",
    "        model (BertForTokenClassification): The fine-tuned BERT model.\n",
    "        tokenizer (BertTokenizer): The tokenizer used during training.\n",
    "        sentence (str): The input sentence.\n",
    "        device (torch.device): The device to use for prediction (GPU or CPU).\n",
    "        max_len (int): Maximum sequence length.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[str, List[Tuple[str, str]]]: A tuple containing the original sentence and a list of word-level predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    # Setting model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Tokenizes sentence\n",
    "    inputs = tokenizer(sentence, padding='max_length', truncation=True, max_length=max_len, return_tensors=\"pt\")\n",
    "    ids = inputs[\"input_ids\"].to(device)\n",
    "    mask = inputs[\"attention_mask\"].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(ids, mask)\n",
    "        logits = outputs.logits  # Use 'logits' instead of 'outputs[0]'\n",
    "\n",
    "    active_logits = logits.view(-1, model.config.num_labels)\n",
    "    flattened_pred = torch.argmax(active_logits, axis=1)\n",
    "\n",
    "    tokens = tokenizer.convert_ids_to_tokens(ids.squeeze().tolist())\n",
    "    token_pred = [model.config.id2label[i] for i in flattened_pred.cpu().numpy()]\n",
    "    wp_preds = list(zip(tokens, token_pred))\n",
    "\n",
    "    word_pred = []\n",
    "    current_label = None\n",
    "    current_word = \"\"\n",
    "\n",
    "    for pair in wp_preds:\n",
    "        if pair[0] in ['[CLS]', '[SEP]', '[PAD]']:\n",
    "            continue\n",
    "        \n",
    "        label=pair[1].split('-')[-1] if '-' in pair[1] else pair[1]\n",
    "\n",
    "        if current_label is None:\n",
    "            current_label=label\n",
    "            current_word=pair[0]\n",
    "        elif label==current_label and current_label !='O':\n",
    "            if pair[0].startswith(\"##\"):\n",
    "                current_word += pair[0][2:]\n",
    "                if current_word in sentence.lower().split(' '):\n",
    "                    original_word_index = sentence.lower().split(' ').index(current_word.lower())\n",
    "                    original_word = sentence.split(' ')[original_word_index]\n",
    "                    current_word = original_word\n",
    "                    \n",
    "            else:\n",
    "                current_word += \" \" + pair[0]\n",
    "\n",
    "        else:\n",
    "            word_pred.append((current_word,current_label))\n",
    "            current_label= label\n",
    "            current_word=pair[0]\n",
    "        \n",
    "    if current_label is not None:\n",
    "        word_pred.append((current_word,current_label))\n",
    "\n",
    "    return word_pred\n",
    "            \n",
    "# Example usage:\n",
    "test_sentence = \"Vanadium oxide nanotubes are promising for gas sensors.\"\n",
    "word_pred = predict_sentence(model, tokenizer, test_sentence, device)\n",
    "\n",
    "#print(word_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
